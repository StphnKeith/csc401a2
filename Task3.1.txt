English Perplexities For MLE and Delta = 0.2, 0.4, 0.6, 0.8, and 1.0
[13.708391831674534, 78.65169349232919, 100.2395020713553, 117.82438833002192, 133.30859483593073, 147.4343489634691]

French Perplexities For MLE and Delta = 0.2, 0.4, 0.6, 0.8, and 1.0
[13.28660126758243, 84.49147349189744, 110.13556988056733, 131.26665794411386, 150.03742072572936, 167.27928524610385]

For both languages as delta increases perplexity increases indicating MLE is the best model in this instance. Delta-smoothing helps to deal with unknown words in the model at the cost of distorting the model somewhat. If the model is judged against a corpus where there are few words unknown to the model in the corpus, then delta-smoothing adds little benefit.